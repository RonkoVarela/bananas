{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep Q-Network\n",
    "\n",
    "### 1. Install and import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !pip install gym[box2d]\n",
    "# !pip install gym[classic_control]\n",
    "# !pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from bananas_agent.dqn_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "BRAIN_NAME = env.brain_names[0]\n",
    "brain = env.brains[BRAIN_NAME]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[BRAIN_NAME]\n",
    "state = env_info.vector_observations[0]\n",
    "ACTION_SIZE = brain.vector_action_space_size\n",
    "STATE_SIZE = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=STATE_SIZE, action_size=ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup train parameters\n",
    "n_episodes = 2000\n",
    "max_steps = 2000\n",
    "eps_max = 1.\n",
    "eps_min = 0.01\n",
    "min_expected_score = -3\n",
    "target_score = 13\n",
    "warm_start = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm start\n",
    "CHECKPOINT_PATH = Path('checkpoints/checkpoint.pth')\n",
    "if warm_start and CHECKPOINT_PATH.is_file():\n",
    "    print(\"Using warm start!\")\n",
    "    agent.qnetwork_local.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "    agent.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, epsilon, max_steps):\n",
    "    \"\"\" Run a single episode. \"\"\"\n",
    "    state = env.reset(train_mode=True)[BRAIN_NAME].vector_observations[0]\n",
    "    score = 0\n",
    "    for t in range(max_steps):\n",
    "        action = agent.act(state, epsilon)\n",
    "        env_info = env.step(action)[BRAIN_NAME]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    return score\n",
    "\n",
    "def epsilon_interpolator(current_score):\n",
    "    \"\"\" Update epsilon with a log interpolation based on average score. \"\"\"\n",
    "    eps_max_log, eps_min_log = np.log(eps_max), np.log(eps_min)\n",
    "    return np.e ** np.interp(\n",
    "        current_score,\n",
    "        xp=[min_expected_score, target_score],\n",
    "        fp=[eps_max_log, eps_min_log],\n",
    "        left=eps_max_log,\n",
    "        right=eps_min_log,\n",
    "    )\n",
    "\n",
    "scores = []\n",
    "# compute average score based on last 100 scores\n",
    "scores_window = deque(maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = eps_max\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    score = run_episode(env, epsilon, max_steps)\n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save most recent score\n",
    "    average_score = np.mean(scores_window)\n",
    "    epsilon = epsilon_interpolator(average_score)\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, average_score, epsilon), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, average_score, epsilon))\n",
    "    if average_score >= target_score:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the score evolution\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "fig.savefig('score_evolution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.qnetwork_local.state_dict(), CHECKPOINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
